{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Skill Extraction**\n",
    "\n",
    "### **Data Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the Datasets\n",
    "job_postings = pd.read_csv('../data/cleaned_job_postings.csv')\n",
    "job_skills = pd.read_csv('../data/cleaned_job_skills.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Merge the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge left on job_postings and job_skills on the 'job_link' column\n",
    "merged_data = job_postings.merge(job_skills, on='job_link', how='left')\n",
    "\n",
    "# Drop the 'job_link' column\n",
    "merged_data = merged_data.drop(columns=['job_link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split the job_skills column into a list of skills**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the job_skills column into a list of skills\n",
    "merged_data['job_skills'] = merged_data['job_skills'].str.split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explode the Job_Skills Column and Remove any Null or Empty**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the Split job_skills\n",
    "skill_breakdown = merged_data.explode('job_skills')\n",
    "\n",
    "# Remove any Null or Empty Skills\n",
    "skill_breakdown = skill_breakdown.dropna(subset=['job_skills'])\n",
    "skill_breakdown = skill_breakdown[skill_breakdown['job_skills'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Standardize / Categorize Skills**\n",
    "\n",
    "#### **Method 1 - Direct Skill Standardization / Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to dynamically map skills based on keywords\n",
    "def dynamic_skill_mapping(skill):\n",
    "\n",
    "    # Standardizing Education-Related Skills\n",
    "\n",
    "    bachelor_variations = [\n",
    "        \"bachelor of\", \"bachelor's\", \"ba\", \"bs\", \"b.a.\", \"b.s.\",\n",
    "        \"bachelor\", \"bachlor\", \"bacs\", \"bsc\", \"b.sc.\"\n",
    "    ]\n",
    "    \n",
    "    master_variations = [\n",
    "        \"master of\", \"master's\", \"ma\", \"ms\", \"m.a.\", \"m.s.\",\n",
    "        \"master\", \"mphil\", \"m.phil.\", \"mpa\", \"m.p.a.\", \"msc\", \"m.sc.\"\n",
    "    ]\n",
    "    \n",
    "    phd_variations = [\n",
    "        \"phd\", \"doctorate\", \"doctor of philosophy\", \"doctor's degree\",\n",
    "        \"ph.d.\", \"phd.\", \"dr\", \"dr.\", \"doctor\"\n",
    "    ]\n",
    "\n",
    "    for variation in bachelor_variations:\n",
    "        if variation in skill:\n",
    "            return \"bachelor's degree\"\n",
    "    \n",
    "    for variation in master_variations:\n",
    "        if variation in skill:\n",
    "            return \"master's degree\"\n",
    "    \n",
    "    for variation in phd_variations:\n",
    "        if variation in skill:\n",
    "            return \"phd\"\n",
    "\n",
    "    # Grouping AWS Certifications into Categories\n",
    "    if \"aws cloud practitioner\" in skill:\n",
    "        return \"AWS Fundamentals\"\n",
    "    if \"developer\" in skill and \"aws\" in skill:\n",
    "        return \"AWS Developer Certifications\"\n",
    "    if \"architect\" in skill and \"aws\" in skill:\n",
    "        return \"AWS Architecture Certifications\"\n",
    "    if \"devops\" in skill and \"aws\" in skill:\n",
    "        return \"AWS DevOps Certifications\"\n",
    "    if \"sysops\" in skill and \"aws\" in skill:\n",
    "        return \"AWS SysOps Administrator\"\n",
    "    \n",
    "    # Standardizing Data Science & Machine Learning Skills\n",
    "    if \"machine learning\" in skill or \"ml engineer\" in skill:\n",
    "        return \"machine learning\"\n",
    "    if \"deep learning\" in skill:\n",
    "        return \"deep learning\"\n",
    "    if \"natural language processing\" in skill or \"nlp\" in skill:\n",
    "        return \"natural language processing\"\n",
    "\n",
    "    # Cloud & Infrastructure Skills\n",
    "    if \"gcp\" in skill or \"google cloud\" in skill:\n",
    "        return \"Google Cloud Platform\"\n",
    "    \n",
    "    # Business Intelligence & Data Analysis\n",
    "    if \"power bi\" in skill:\n",
    "        return \"Power BI\"\n",
    "    if \"tableau\" in skill:\n",
    "        return \"Tableau\"\n",
    "    if \"excel\" in skill or \"spreadsheet\" in skill:\n",
    "        return \"Excel\"\n",
    "    \n",
    "    # Programming Languages\n",
    "    if \"python\" in skill:\n",
    "        return \"Python\"\n",
    "    if \"r programming\" in skill or skill == \"r\":\n",
    "        return \"R Programming\"\n",
    "    if \"java\" in skill:\n",
    "        return \"Java\"\n",
    "    if \"javascript\" in skill or \"js\" in skill:\n",
    "        return \"JavaScript\"\n",
    "    if \"c++\" in skill or \"c plus plus\" in skill:\n",
    "        return \"C++\"\n",
    "\n",
    "    # Databases\n",
    "    if \"postgresql\" in skill or \"postgre\" in skill:\n",
    "        return \"PostgreSQL\"\n",
    "    if \"mongodb\" in skill:\n",
    "        return \"MongoDB\"\n",
    "    \n",
    "    if \"$\" in skill or \"hour\" in skill or \"day\" in skill:\n",
    "        return None\n",
    "    \n",
    "    if \"relevant\" in skill or \"related\" in skill:\n",
    "        return None\n",
    "\n",
    "    if \"24/7\" in skill or \"24x7\" in skill:\n",
    "        return None\n",
    "\n",
    "    if \"401k\" in skill or \"401(k)\" in skill or \"retirement\" in skill:\n",
    "        return None   \n",
    "    \n",
    "    if \"*\" in skill:\n",
    "        return skill[1:].strip()\n",
    "    \n",
    "    if \"'big data'\" in skill:\n",
    "        return \"big data\"\n",
    "    \n",
    "    if \"years experience\" in skill or \"years of experience\" in skill or \"years'\" in skill or \"year of\" in skill or \"year\" in skill:\n",
    "        return None\n",
    "    \n",
    "    if \"a/b\" in skill:\n",
    "        return \"a/b testing\"\n",
    "    \n",
    "    if \"ability\" in skill:\n",
    "        return None\n",
    "    \n",
    "    if \"bsa\" in skill:\n",
    "        return \"bsa/aml\"\n",
    "    \n",
    "    if \"data engineering\" in skill:\n",
    "        return \"data engineering\"\n",
    "    \n",
    "    if \"data entry\" in skill:\n",
    "        return \"data entry\"\n",
    "    \n",
    "    if \"etl\" in skill:\n",
    "        return \"ETL\"\n",
    "\n",
    "    # Default: Return skill as is if no match is found\n",
    "    return skill\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Function Execution and Clean Up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize job_skills to lowercase and remove extra spaces\n",
    "skill_breakdown[\"lowercase_skills\"] = skill_breakdown[\"job_skills\"].str.lower().str.strip()\n",
    "\n",
    "# Apply the dynamic skill mapping function to the unique skills list\n",
    "cleaned_unique_skills = [dynamic_skill_mapping(skill) for skill in skill_breakdown[\"lowercase_skills\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Row Count: 150000\n",
      "Skill Breakdown Row Count After Cleaning: 150000\n"
     ]
    }
   ],
   "source": [
    "# Row Count Check:\n",
    "print (f\"\"\"Original Row Count: {len(skill_breakdown)}\n",
    "Skill Breakdown Row Count After Cleaning: {len(cleaned_unique_skills)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the job_skills column with the cleaned skills\n",
    "skill_breakdown[\"job_skills\"] = cleaned_unique_skills\n",
    "\n",
    "# Drop the lowercase_skills column\n",
    "skill_breakdown = skill_breakdown.drop(columns=[\"lowercase_skills\"])\n",
    "\n",
    "# Drop Null or Empty Rows\n",
    "skill_breakdown = skill_breakdown.dropna(subset=['job_skills'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **MVP Categorically Common Skill Standardization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skill Normalization by Ranking Priority\n",
    "skill_normalization = {\n",
    "    \n",
    "    # Classification Control to limit clash of the multiple Skill Standardizations (Refer to Jose's Section)\n",
    "    \n",
    "    \"bachelor's degree\": r\"(?i)\\bbachelor's degree\\b\",\n",
    "    \"master's degree\": r\"(?i)\\bmaster's degree\\b\",\n",
    "    \"phd\": r\"(?i)\\bphd\\b\",\n",
    "    \"AWS Fundamentals\": r\"(?i)\\bAWS Fundamentals\\b\",\n",
    "    \"AWS Developer Certifications\": r\"(?i)\\bAWS Developer Certifications\\b\",\n",
    "    \"AWS Architecture Certifications\": r\"(?i)\\bAWS Architecture Certifications\\b\",\n",
    "    \"AWS DevOps Certifications\": r\"(?i)\\bAWS DevOps Certifications\\b\",\n",
    "    \"AWS SysOps Administrator\": r\"(?i)\\bAWS SysOps Administrator\\b\",\n",
    "    \n",
    "    # Cloud & Infrastructure Engineering:\n",
    "    \n",
    "    \"Troubleshooting\": r\"(?i)troubleshooting\",\n",
    "    \"Linux\": r\"(?i)linux\",\n",
    "    \"Communication\": r\"(?i)communication|communication\\s+skills|comm|comms|communicate\",\n",
    "    \"Cabling\": r\"(?i)cable[-\\s]*ing\",\n",
    "    \"Networking\": r\"(?i)networking\",\n",
    "    \"Data Center Operations\": r\"(?i)data\\s+center\\s+operations\",\n",
    "    \"Windows\": r\"(?i)windows\",\n",
    "    \"Azure Data Factory\": r\"(?i)azure\\s+data\\s+factory|azuredatafactory\",\n",
    "    \"Terraform\": r\"(?i)terraform\",\n",
    "    \"Bash\": r\"(?i)(bash|shell|command\\s+line|cli)\",\n",
    "    \"Big Data\": r\"(?i)big\\s+data|bigdata\",\n",
    "    \"Data Engineering\": r\"(?i)data\\s+engineering|dataeng|data\\sengg|data\\sengr\",\n",
    "    \"Databricks\": r\"(?i)databricks\",\n",
    "    \"Go\": r\"(?i)go|golang\",\n",
    "    \"Inventory Management\": r\"(?i)inventory\\s+management|inv\\s+mgmt|inv\\s+mgmnt\",\n",
    "    \n",
    "    # Data Analyst:\n",
    "    \n",
    "    \"Data Analysis\": r\"(?i)data\\s*(analysis|analytics)|data\\s*analyse\",\n",
    "    \"Tableau\": r\"(?i)tableau\",\n",
    "    \"Data Visualization\": r\"(?i)data\\s+visualization|data\\s+visualisation\",\n",
    "    \"Excel\": r\"(?i)excel\",\n",
    "    \"Power BI\": r\"(?i)power\\s*bi\",\n",
    "    \"Statistics\": r\"(?i)statistics|statistical\",\n",
    "    \"Reporting\": r\"(?i)reporting|reports\",\n",
    "    \"Teamwork\": r\"(?i)teamwork|collaboration|team-\\s*first\\s*mentality\",\n",
    "    \"Data Mining\": r\"(?i)data\\s+mining|mining\",\n",
    "    \"Problem Solving\": r\"(?i)problem\\s+solving|troubleshooting\",\n",
    "    \"Business Intelligence\": r\"(?i)business\\s*intelligence|bi\",\n",
    "    \"Project Management\": r\"(?i)project\\s*management\",\n",
    "    \"Data Management\": r\"(?i)data\\s+management\",\n",
    "    \n",
    "    # Data Architect:\n",
    "    \n",
    "    \"Data Architecture\": r\"(?i)data\\s*(architecture|architect)\",\n",
    "    \"Data Modeling\": r\"(?i)data\\s*modeling|data\\s*models|data\\s*design\",\n",
    "    \"Data Warehousing\": r\"(?i)data\\s*warehousing|data\\s*marts|dw\",\n",
    "    \"Data Governance\": r\"(?i)data\\s*governance|data\\s*policy|data\\s*compliance\",\n",
    "    \"Snowflake\": r\"(?i)snowflake\",\n",
    "    \"Data Integration\": r\"(?i)data\\s*integration|etl|extract\\s+transform\\s+load|data\\s*flow\",\n",
    "    \"Data Quality\": r\"(?i)data\\s*quality|dq\",\n",
    "    \"Hadoop\": r\"(?i)hadoop\",\n",
    "    \"Data Security\": r\"(?i)data\\s*security|data\\s*protection|cybersecurity\",\n",
    "    \"ETL\": r\"(?i)etl|extract\\s+transform\\s+load|data\\s*flow\",\n",
    "    \"NoSQL\": r\"(?i)nosql\",\n",
    "    \n",
    "    #Data Engineer:\n",
    "    \n",
    "    \"Spark\": r\"(?i)spark|apache\\s+spark\",\n",
    "    \"Scala\": r\"(?i)scala\",\n",
    "    \"Kafka\": r\"(?i)kafka|apache\\s+kafka\",\n",
    "    \"Redshift\": r\"(?i)redshift|amazon\\s+redshift\",\n",
    "    \"NoSQL\": r\"(?i)nosql\",\n",
    "    \"Hive\": r\"(?i)hive|apache\\s+hive\",\n",
    "    \"ETL\": r\"(?i)etl|extract\\s+transform\\s+load\",\n",
    "    \"MySQL\": r\"(?i)mysql\",\n",
    "    \"Agile\": r\"(?i)agile\",\n",
    "    \"EMR\": r\"(?i)emr|elastic\\s+mapreduce|amazon\\s+emr\",\n",
    "    \"Airflow\": r\"(?i)airflow|apache\\s+airflow\",\n",
    "    \"Cassandra\": r\"(?i)cassandra|apache\\s+cassandra\",\n",
    "    \n",
    "    # Data Governance & Security:\n",
    "    \n",
    "    \"Data Governance\": r\"(?i)data\\s*governance|dg\",\n",
    "    \"Data Privacy\": r\"(?i)data\\s+privacy\",\n",
    "    \"Analytical Skills\": r\"(?i)analytical\\s+skills\",\n",
    "    \"Computer Science\": r\"(?i)computer\\s+science|cs\",\n",
    "    \"Data Protection\": r\"(?i)data\\s+protection|data\\s*prot|dp\",\n",
    "    \"Data Stewardship\": r\"(?i)data\\s*stewardship\",\n",
    "    \"GIS\": r\"(?i)gis|geographic\\sinformation\\s+systems\",\n",
    "    \"GDPR\": r\"(?i)gdpr|general\\+data\\s+protection\\s+regulation\",\n",
    "    \n",
    "    # Data Modeling and Warehousing:\n",
    "    \n",
    "    \"JSON\": r\"(?i)json\",\n",
    "    \"SPARQL\": r\"(?i)sparql\",\n",
    "    \"AVRO\": r\"(?i)avro\",\n",
    "    \"Ontology\": r\"(?i)ontology\",\n",
    "    \"OpenAPI/YAML\": r\"(?i)openapi/yaml\",\n",
    "    \"OWL\": r\"(?i)owl\",\n",
    "    \"SKOS\": r\"(?i)skos\",\n",
    "    \"Data.World\": r\"(?i)data\\.world\",\n",
    "    \"RDFS\": r\"(?i)rdfs\",\n",
    "    \"Stardog\": r\"(?i)stardog\",\n",
    "    \"AnzoGraph\": r\"(?i)anzograph\",\n",
    "    \"Neptune\": r\"(?i)neptune\",\n",
    "    \"PoolParty\": r\"(?i)poolparty\",\n",
    "    \n",
    "    # Data Operations & management: engineering\n",
    "    \n",
    "    'Collaboration': r\"(?i)collaboration\",\n",
    "    'Attention to Detail': r\"(?i)attention\\s*to\\s*detail\",\\\n",
    "    'Microsoft Office Suite': r\"(?i)microsoft\\s*office(?:\\ssuite)?\",\n",
    "    'Data Validation': r\"(?i)data\\s*validation\",\n",
    "    \n",
    "    # Data Scientist:\n",
    "    \n",
    "    'Data Science': r\"(?i)data\\s*science\",\n",
    "    'Machine Learning': r\"(?i)machine\\s*learning|ml\",\n",
    "    'Mathematics': r\"(?i)mathematics|maths\",\n",
    "    'PyTorch': r\"(?i)pytorch\",\n",
    "    \n",
    "    # Data Specialist:\n",
    "    \n",
    "    'Data Entry': r\"(?i)data\\s*entry\",\n",
    "    'Multitasking': r\"(?i)multitasking\",\n",
    "    \n",
    "    # Data Engineer / Administrator:\n",
    "    \n",
    "    'Oracle': r\"(?i)oracle\",\n",
    "    'Database Administration': r\"(?i)database\\s*administration|db\\sa|dba\",\n",
    "    'SQL Server': r\"(?i)sql\\s*server\",\n",
    "    'PostgreSQL': r\"(?i)postgresql|postgres\",\n",
    "    'Database Design': r\"(?i)database\\sgesign|db\\sdesign|database\\sstructure\",\n",
    "    'PL/SQL': r\"(?i)pl/sql\",\n",
    "    'MongoDB': r\"(?i)(mongodb|mongo\\s*database)\",\n",
    "    'Performance Tuning': r\"(?i)performance\\stuning|tuning\",\n",
    "    \n",
    "    # ML Ops Engineer:\n",
    "    \n",
    "    'Reinforcement Learning': r\"(?i)reinforcement\\s*learning\",\n",
    "    'Probabilistic Graphs': r\"(?i)probabilistic\\s*graphs\",\n",
    "    'Flexibility': r\"(?i)flexibility\",\n",
    "    'NLP': r\"(?i)(nlp|natural\\s*language\\s*processing)\",\n",
    "    'Monitoring': r\"(?i)monitoring\",\n",
    "    'Autonomy': r\"(?i)autonomy\",\n",
    "    'Experimentation': r\"(?i)experimentation\",\n",
    "    'Deep Learning': r\"(?i)deep\\s*learning\",\n",
    "    'ML Ops': r\"(?i)(ml\\s*ops|mlops|machine\\s*learning\\s*operations)\",\n",
    "    'Workflow Orchestration': r\"(?i)workflow\\s*orchestration\",\n",
    "    'Product Ownership': r\"(?i)product\\s*ownership\",\n",
    "    \n",
    "    # Machine Learning Engineer:\n",
    "    \n",
    "    'TensorFlow': r\"(?i)(tensorflow|tensor\\s*flow)\",\n",
    "    'Pandas': r\"(?i)pandas\",\n",
    "    'Data Preparation': r\"(?i)data\\s*preparation\",\n",
    "    'Jupyter': r\"(?i)jupyter\",\n",
    "    'Numba': r\"(?i)numba\",\n",
    "    \"Cloud Computing\": r\"(?i)cloud\\s+computing|cc\",\n",
    "    'Model Deployment': r\"(?i)model\\s*deployment\",\n",
    "    \"Kubernetes\": r\"(?i)kubernetes|kube\",\n",
    "    'Docker': r\"(?i)docker\",\n",
    "    'Feature Engineering': r\"(?i)feature\\s*engineering\",\n",
    "    \n",
    "    # Risk and Compliance Analyst:\n",
    "    \n",
    "    'CISM': r\"(?i)(certified\\s*information\\s*systems\\s*manager|cism)\",\n",
    "    'JIRA': r\"(?i)jira\",\n",
    "    'CISSP': r\"(?i)(certified\\s*information\\s*systems\\s*security\\s*professional|cissp)\",\n",
    "    'CCSP': r\"(?i)(certified\\s*cloud\\s*security\\s*professional|ccsp)\",\n",
    "    'CISA': r\"(?i)(certified\\s*information\\s*systems\\s*auditor|cisa)\",\n",
    "    'Security+': r\"(?i)(comp\\.?\\s*tia\\ssecurity\\+\\s*certification|security\\+)\",\n",
    "    'GIAC': r\"(?i)giac\",\n",
    "    'AWS Cloud Practitioner': r\"(?i)(aws\\s*cloud\\s*practitioner|awscp)\",\n",
    "    'AWS Solution Architect Associate': r\"(?i)(aws\\s*solution\\s*architect\\s*associate|aws\\ssaa)\",\n",
    "    'AWS Solution Architect Professional': r\"(?i)(aws\\s*solution\\s*architect\\s*professional|aws\\sasap)\",\n",
    "    'AWS Developer Associate': r\"(?i)(aws\\s*developer\\s*associate|aws\\sdaa)\",\n",
    "    'AWS Security Specialty': r\"(?i)(aws\\s*security\\s*specialty|awsss)\",\n",
    "    'Virtualization': r\"(?i)virtualization\",\n",
    "    'Cybersecurity': r\"(?i)cybersecurity\",\n",
    "    'Data Loss Prevention (DLP)': r\"(?i)(data\\s*loss\\s*prevention|dlp)\",\n",
    "    'Network DLP': r\"(?i)(network\\s*dlp|ndlp)\",\n",
    "    'SaaS': r\"(?i)saas\",\n",
    "    \n",
    "    # Software and Platform Engineering:\n",
    "    \n",
    "    \"Software Engineering\": r\"(?i)software\\s+engineering\",\n",
    "    \"Kafka\": r\"(?i)kafka\",\n",
    "    \"C++\": r\"(?i)c\\+\\+|\\bCPLUSPLUS\\b\",\n",
    "    \"Algorithms\": r\"(?i)algorithms|algo\",\n",
    "    \"CI/CD\": r\"(?i)ci/cd|continuous\\s+integration/\\s*continuous\\s+deployment\",\n",
    "    \"AI\": r\"(?i)ai|artificial\\s+intelligence\",\n",
    "    \n",
    "    # General / Added:\n",
    "    \n",
    "    'Engineering': r\"(?i)engineering\",\n",
    "    'LLMs': r\"(?i)(llms|large\\s*language\\s*models)\",\n",
    "    \"Python\": r\"(?i)python(?:3(\\.\\d+)?)?|py\",\n",
    "    \"RDF\": r\"(?i)rdf\",\n",
    "    \"AWS\": r\"(?i)aws|amazon\\s+web\\sservices\",\n",
    "    \"SQL\": r\"(?i)sql\",\n",
    "    \"Azure\": r\"(?i)azure|microsft\\s+azure\",\n",
    "    \"Java\": r\"(?i)java|java\\s+ee|java\\s+se\",\n",
    "    \"R\": r\"(?i)(?:^|[\\s,])(r(?:\\s+(?:programming|language|studio|basics|core|developer|development|statistical|stats|analysis))?)\\b\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Classification Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function To Standardize top 20 Skills by Job Classification\n",
    "def classify(skill):\n",
    "    for skill_name, keyword in skill_normalization.items():\n",
    "        if re.search(keyword, skill):\n",
    "            return skill_name\n",
    "    return skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Regex Function to skill_breakdown Dataframe\n",
    "skill_breakdown['job_skills'] = skill_breakdown['job_skills'].apply(classify)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implode skill_breakdown dataframe on Skills**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = skill_breakdown.groupby(['job_id', 'last_processed_time', 'job_title', 'company', 'City', 'State', 'title_id',\n",
    "                        'job_classification', 'keyword_id', 'job_keyword', 'seniority_level', 'seniority_level_keyword'], as_index=False).agg(\n",
    "    job_skills=('job_skills', list)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>last_processed_time</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>title_id</th>\n",
       "      <th>job_classification</th>\n",
       "      <th>keyword_id</th>\n",
       "      <th>job_keyword</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>seniority_level_keyword</th>\n",
       "      <th>job_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-21 08:08:48.031964+0000</td>\n",
       "      <td>Senior Machine Learning Engineer</td>\n",
       "      <td>Jobs for Humanity</td>\n",
       "      <td>New Haven</td>\n",
       "      <td>CT</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>Senior-Level</td>\n",
       "      <td>Senior</td>\n",
       "      <td>[master's degree, programming, Python, Scala, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-20 04:02:12.331406+0000</td>\n",
       "      <td>Principal Software Engineer, ML Accelerators</td>\n",
       "      <td>Aurora</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>2</td>\n",
       "      <td>Software &amp; Platform Engineering</td>\n",
       "      <td>2</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Principal / Staff-Level</td>\n",
       "      <td>Principal</td>\n",
       "      <td>[C++, Python, PyTorch, TensorFlow, mxnet, cuda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2024-01-21 08:08:31.941595+0000</td>\n",
       "      <td>Senior ETL Data Warehouse Specialist</td>\n",
       "      <td>Adame Services LLC</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Modeling &amp; Warehousing</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Warehouse</td>\n",
       "      <td>Senior-Level</td>\n",
       "      <td>Senior</td>\n",
       "      <td>[Data Integration, Data Integration, master's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2024-01-20 15:30:55.796572+0000</td>\n",
       "      <td>Senior Data Warehouse Developer / Architect</td>\n",
       "      <td>Morph Enterprise</td>\n",
       "      <td>Harrisburg</td>\n",
       "      <td>PA</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Modeling &amp; Warehousing</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Warehouse</td>\n",
       "      <td>Senior-Level</td>\n",
       "      <td>Senior</td>\n",
       "      <td>[data lakes, data bricks, Azure Data Factory, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2024-01-21 08:08:58.312124+0000</td>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Dice</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>4</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>4</td>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead</td>\n",
       "      <td>[Java, Scala, Python, master's degree, NoSQL, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_id              last_processed_time  \\\n",
       "0       1  2024-01-21 08:08:48.031964+0000   \n",
       "1       2  2024-01-20 04:02:12.331406+0000   \n",
       "2       3  2024-01-21 08:08:31.941595+0000   \n",
       "3       4  2024-01-20 15:30:55.796572+0000   \n",
       "4       5  2024-01-21 08:08:58.312124+0000   \n",
       "\n",
       "                                      job_title             company  \\\n",
       "0              Senior Machine Learning Engineer   Jobs for Humanity   \n",
       "1  Principal Software Engineer, ML Accelerators              Aurora   \n",
       "2          Senior ETL Data Warehouse Specialist  Adame Services LLC   \n",
       "3   Senior Data Warehouse Developer / Architect    Morph Enterprise   \n",
       "4                            Lead Data Engineer                Dice   \n",
       "\n",
       "            City State  title_id               job_classification  keyword_id  \\\n",
       "0      New Haven    CT         1        Machine Learning Engineer           1   \n",
       "1  San Francisco    CA         2  Software & Platform Engineering           2   \n",
       "2       New York    NY         3      Data Modeling & Warehousing           3   \n",
       "3     Harrisburg    PA         3      Data Modeling & Warehousing           3   \n",
       "4          Plano    TX         4                    Data Engineer           4   \n",
       "\n",
       "                 job_keyword          seniority_level seniority_level_keyword  \\\n",
       "0  Machine Learning Engineer             Senior-Level                  Senior   \n",
       "1          Software Engineer  Principal / Staff-Level               Principal   \n",
       "2             Data Warehouse             Senior-Level                  Senior   \n",
       "3             Data Warehouse             Senior-Level                  Senior   \n",
       "4         Lead Data Engineer                     Lead                    Lead   \n",
       "\n",
       "                                          job_skills  \n",
       "0  [master's degree, programming, Python, Scala, ...  \n",
       "1  [C++, Python, PyTorch, TensorFlow, mxnet, cuda...  \n",
       "2  [Data Integration, Data Integration, master's ...  \n",
       "3  [data lakes, data bricks, Azure Data Factory, ...  \n",
       "4  [Java, Scala, Python, master's degree, NoSQL, ...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convert and Output the Cleaned result_df as a split JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split CSV into 3 files\n",
    "title_classifications_df = result_df[['title_id', 'job_classification']].drop_duplicates(subset='title_id')\n",
    "keyword_classifications_df = result_df[['keyword_id', 'job_keyword']].drop_duplicates(subset='keyword_id')\n",
    "job_postings_df = result_df[['job_id', 'last_processed_time', 'job_title', 'company', 'City', 'State', 'title_id', 'keyword_id', 'seniority_level', 'seniority_level_keyword', 'job_skills']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the result_df into a job_postings_json, title_classifications_json, and keyword_classifications_json\n",
    "title_classifications_json = json.loads(title_classifications_df.to_json(orient='records'))\n",
    "keyword_classifications_json = json.loads(keyword_classifications_df.to_json(orient='records'))\n",
    "job_postings_json = json.loads(job_postings_df.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_classifications CSV has been converted to JSON successfully!\n",
      "keyword_classifications_json CSV has been converted to JSON successfully!\n",
      "job_postings_json CSV has been converted to JSON successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save JSON to a file\n",
    "with open(\"../data/title_classifications.json\", \"w\") as json_file:\n",
    "    json_file.write(json.dumps(title_classifications_json, indent=4))\n",
    "print(\"title_classifications CSV has been converted to JSON successfully!\")    \n",
    "\n",
    "with open(\"../data/keyword_classifications.json\", \"w\") as json_file:\n",
    "    json_file.write(json.dumps(keyword_classifications_json, indent=4))\n",
    "print(\"keyword_classifications_json CSV has been converted to JSON successfully!\")    \n",
    "\n",
    "with open(\"../data/job_postings.json\", \"w\") as json_file:\n",
    "    json_file.write(json.dumps(job_postings_json, indent=4))\n",
    "print(\"job_postings_json CSV has been converted to JSON successfully!\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
