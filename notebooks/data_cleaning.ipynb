{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Cleaning**\n",
    "### **Data Ingestion**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "job_postings = pd.read_csv('../data/job_postings.csv')\n",
    "job_skills = pd.read_csv('../data/job_skills.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing Null or Missing Rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate rows with null or missing values\n",
    "cleaned_job_postings = job_postings.dropna()\n",
    "cleaned_job_skills = job_skills.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Eliminating Unnecessary Columns and Fixing any Necessary Ones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate unnecessary columns\n",
    "columns_to_drop = ['last_status', 'got_summary', 'got_ner', 'is_being_worked', 'first_seen', 'job_type', 'search_city', 'search_position', 'job_level', 'search_country']\n",
    "cleaned_job_postings = cleaned_job_postings.drop(columns=[col for col in columns_to_drop if col in cleaned_job_postings.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix last_processed_time \n",
    "cleaned_job_postings[\"last_processed_time\"] = cleaned_job_postings[\"last_processed_time\"].str.replace(\"+00\", \"+0000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing Duplicate Rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 752\n"
     ]
    }
   ],
   "source": [
    "# Eliminate duplicate rows\n",
    "columns_to_check = ['job_title', 'company', 'job_location']\n",
    "\n",
    "print(\"Number of duplicates:\", \n",
    "        cleaned_job_postings.duplicated(subset=columns_to_check).sum())\n",
    "\n",
    "cleaned_job_postings = cleaned_job_postings.drop_duplicates(subset=columns_to_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Splitting the job_location Collumn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_job_postings[['City', 'State']] = cleaned_job_postings['job_location'].str.split(', ', n=1, expand = True)\n",
    "cleaned_job_postings = cleaned_job_postings.drop(columns='job_location')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove Non US Standardized Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of Valid US State Abbreviations\n",
    "valid_states = [\n",
    "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\",\n",
    "    \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\",\n",
    "    \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\n",
    "    \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\n",
    "    \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all Rows that dont have the Standardized US State Abbreviations\n",
    "cleaned_job_postings = cleaned_job_postings[cleaned_job_postings['State'].isin(valid_states)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Title Classification**\n",
    "### **Target Jobs Classification Regex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target job title regex list\n",
    "target_job_titles_regex = {\n",
    "    \"MLOps Engineer\": r\"(?i)(MLOps|Machine Learning Operations|Machine Learning Infrastructure Engineer|ML Infrastructure|ML Platform|ML Systems|ML Platform Engineer|AIML Ops Engineer|Machine Learning Software Developer)\\w*[-\\s]?\",\n",
    "\n",
    "    \"Machine Learning Engineer\": r\"(?i)(Machine Learning Engineer|ML Engineer|Machine Learning Engineering|ML Developer|Machine Learning Software Engineer|AIML Engineer|AIML Data Scientist|AI Data Science Lead)\\w*[-\\s]?\",\n",
    "\n",
    "    \"Data Architect\": r\"(?i)(Data Architect|Senior Data Architect|Cloud Data Architect|Big Data Architect|Enterprise Data Architect|Principal Data Architect|Lead Data Architect|Data Warehouse Architect|Data Architecture|Data Lake Architect|Data Streaming Architect)\\w*[-\\s]?\",\n",
    "\n",
    "    \"Database Engineer / Administrator\": r\"(?i)(Database|Database Architect|DBA\\b|Cloud Database|Azure Database|AWS Database|Databases|GCP Database|Oracle Database Engineer)\\w*[-\\s]?\",\n",
    "\n",
    "    \"Data Engineer\": r\"(?i)(Data Engineer|Senior Data Engineer|Lead Data Engineer|Big Data Engineer|Data Engineering|Data Engineering Manager|Data Engineering Architect|Data Pipeline Engineer|Big Data Developer|Data Engineers|Data Integrations|Data Infrastructure|ETL Developer)\\w*[-\\s]?\",\n",
    "\n",
    "    \"Data Governance & Security\": r\"(?i)(Data Governance|Data Privacy|Data Steward|Data Protection|Data Security|Master Data Management|Data Governance Manager|Data Compliance|Data Lifecycle Manager)\\w*[-\\s]?\",\n",
    "\n",
    "    \"Data Operations & Management\": r\"(?i)(Data Manager|Enterprise Data Manager|Data Operations|Data Operations Manager|Data Operations Analyst|Data Management Engineer|Data Strategy Manager|Data Solution Architect|Data Deployment|Data Conversion|Data Replication Engineer|DevOps Engineer|Distributed Systems|Storage)\\w*[-\\s]?\",\n",
    "\n",
    "    \"Data Modeling & Warehousing\": r\"(?i)(Data Modeling|Data Warehouse|Big Data Developer|Data Warehouse Architect|Cloud Datawarehouse|Data Platform Developer)\\w*[-\\s]?\",\n",
    "\n",
    "    \"Data Specialist\": r\"(?i)(Data Specialist|Data Processing|Data Consultant|Data Quality Manager|Data Coordinator|Data Entry Specialist)\\w*[-\\s]?\",\n",
    "\n",
    "    \"Data Scientist\": r\"(?i)(Data Scientist|Data Scientists|Data Science Engineer|Data Science Manager|Data Science Analyst|Data Science Practitioner|Customer Data Scientist)\\w*[-\\s]?\",\n",
    "\n",
    "    \"Data Analyst\": r\"(?i)(Data Analyst|Data Analysts|Financial Data Analyst|Business Intelligence|BI Analyst|Data Business Analyst|Data Insights Analyst)\\w*[-\\s]?\",\n",
    "\n",
    "    \"Software & Platform Engineering\": r\"(?i)(Software Engineer|Software Engineering|Software Developer|Software Engineer Data Science|Software Engineer Data Platforms|Platform Engineer|Application Developer|Backend Engineer|Systems Developer)\\w*[-\\s]?\",\n",
    "\n",
    "    \"Cloud & Infrastructure Engineering\": r\"(?i)(Cloud Data|Cloud Data Architect|Azure Data|AWS Data|Azure Databricks|AWS Databricks|Cloud Engineer|Cloud Platform Engineer|Infrastructure Engineer|Datacenter Technician|Datacenter Engineer|Datacenter Network Engineer|Datacenter Engineering|Site Reliability Engineer|SRE)\\w*[-\\s]?\",\n",
    "\n",
    "    \"Risk & Compliance Analytics\": r\"(?i)(Risk Analyst|AML\\b|BSA|Risk Modeling|Financial Analyst|Hedge Fund|Data Loss Prevention|DLP)\\w*[-\\s]?\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Classification Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Classify Job Titles\n",
    "def classify(job_title, keywords_list=target_job_titles_regex):\n",
    "    for industry, keyword in keywords_list.items():\n",
    "        match = re.search(keyword, str(job_title))\n",
    "        if match:\n",
    "            keyword = re.sub(r'[^a-zA-Z\\s]', '', match.group()).strip().title()   # using match.group() to return the actual keyword that was matched rather than the regex pattern\n",
    "            return industry, keyword              \n",
    "    return \"unclassified\", \"unclassified\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Dataframe and Execute the classification function\n",
    "classified_job_titles = cleaned_job_postings.copy()\n",
    "classified_job_titles['job_classification'], classified_job_titles['job_keyword'] = zip(*classified_job_titles['job_title'].apply(classify))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Seniority Level Classification Regex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seniority level regex list\n",
    "seniority_levels_regex = {\n",
    "    # ðŸ”¹ Principal / Staff-Level Roles (Must Be Checked First)\n",
    "    \"Principal / Staff-Level\": r\"(?i)(Principal|Staff|Sr[-\\s]?Staff|Distinguished|Fellow|Master|L4|Level 4|Chief[-\\s]?Architect|Chief[-\\s]?Scientist)\\w*[-\\s]?\",\n",
    "\n",
    "    # ðŸ”¹ Lead / Supervisor Roles (Checked Before Senior)\n",
    "    \"Lead\": r\"(?i)(Lead|Tech[-\\s]?Lead|Team[-\\s]?Lead|Supervisor|Group[-\\s]?Lead|Project[-\\s]?Lead|Engineering[-\\s]?Lead|Squad[-\\s]?Lead|Chapter[-\\s]?Lead|Manager|Head[-\\s]?of[-\\s]?Team)\\w*[-\\s]?\",\n",
    "\n",
    "    # ðŸ”¹ Senior-Level Roles (Checked Before Mid-Level)\n",
    "    \"Senior-Level\": r\"(?i)(Senior|Sr\\.?|SNR|SEN|L3|Level 3|Expert|Specialist|Advanced|Seasoned|Experienced)\\w*[-\\s]?\",\n",
    "\n",
    "    # ðŸ”¹ Mid-Level Roles (Checked Before Junior)\n",
    "    \"Mid-Level\": r\"(?i)(Mid[-\\s]?Level|Intermediate|Mid|L2|Level 2|Professional|Regular)\\w*[-\\s]?\",\n",
    "\n",
    "    # ðŸ”¹ Entry-Level / Junior Roles (Checked After Principal & Senior)\n",
    "    \"Entry-Level / Junior\": r\"(?i)(Junior|Jr\\.?|Entry[-\\s]?Level|Associate|Graduate|Trainee|Fresher|New Grad|Early[-\\s]?Career|L1|Level 1)\\w*[-\\s]?\",\n",
    "\n",
    "    # ðŸ”¹ Intern / Internship Roles (Checked Last)\n",
    "    \"Intern\": r\"(?i)(Intern|Internship|Co[-\\s]?Op|Apprentice|Trainee)\\w*[-\\s]?\",\n",
    "\n",
    "    # ðŸ”¹ Director / Executive Roles (Checked Last for Highest Priority)\n",
    "    \"Director / Executive\": r\"(?i)(Director|Head|VP|Vice[-\\s]?President|CIO|CTO|CISO|CEO|Chief|Executive|C[-]?Level|Managing[-\\s]?Director|Global[-\\s]?Head|President|Founder|Partner)\\w*[-\\s]?\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Seniority Level Classification Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create single use function to classify seniority level\n",
    "def classify_seniority_level(job_title):\n",
    "    return classify(job_title, seniority_levels_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the classification function on the copied dataframe\n",
    "classified_job_titles['seniority_level'], classified_job_titles['seniority_level_keyword'] = zip(*classified_job_titles['job_title'].apply(classify_seniority_level))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Classify the Unclassified Seniority Titles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a For Loop and If Conditional to classify the unclassified seniority titles\n",
    "for index, row in classified_job_titles.iterrows():\n",
    "    if row['seniority_level'] == 'unclassified':\n",
    "        if row['job_keyword'] == 'Data Analyst' or row['job_keyword'] == 'Data Security' or row['job_keyword'] == 'Database' or row['job_keyword'] == 'Cloud Engineer' or \\\n",
    "                row['job_keyword'] == 'Financial Data Analyst' or row['job_keyword'] == 'Bsa' or row['job_keyword'] == 'Machine Learning Engineer' or row['job_keyword'] == 'Data Processing' or \\\n",
    "                row['job_keyword'] == 'Backend Engineer' or row['job_keyword'] == 'Ml Engineering' or row['job_keyword'] == 'Data Governance' or row['job_keyword'] == 'Big Data Engineer' or \\\n",
    "                row['job_keyword'] == 'Aml' or row['job_keyword'] == 'Data Privacy' or row['job_keyword'] == 'Data Business Analyst' or row['job_keyword'] == 'Data Engineers' or \\\n",
    "                row['job_keyword'] == 'Data Engineer' or row['job_keyword'] == 'Infrastructure Engineer' or row['job_keyword'] == 'Datacenter Technician' or \\\n",
    "                row['job_keyword'] == 'Data Operations' or row['job_keyword'] == 'Data Science Engineer' or row['job_keyword'] == 'Data Consultant' or \\\n",
    "                row['job_keyword'] == 'Software Developer' or row['job_keyword'] == 'Data Science Analyst' or row['job_keyword'] == 'Bi Analyst' or \\\n",
    "                row['job_keyword'] == 'Ml Developer' or row['job_keyword'] == 'Ml Engineer' or row['job_keyword'] == 'Datacenter Engineer' or row['job_keyword'] == 'Platform Engineer' or \\\n",
    "                row['job_keyword'] == 'Cloud Data' or row['job_keyword'] == 'Etl Developer' or row['job_keyword'] == 'Dba' or row['job_keyword'] == 'Databases' or \\\n",
    "                row['job_keyword'] == 'Financial Analyst' or row['job_keyword'] == 'Devops Engineer' or row['job_keyword'] == 'Data Insights Analyst' or \\\n",
    "                row['job_keyword'] == 'Risk Analyst' or row['job_keyword'] == 'Data Analysts' or row['job_keyword'] == 'Cloud Database' or \\\n",
    "                row['job_keyword'] == 'Site Reliability Engineer' or row['job_keyword'] == 'Data Analystat' or row['job_keyword'] == 'Data Pipeline Engineer' or \\\n",
    "                row['job_keyword'] == 'Big Data Engineering':\t\n",
    "            classified_job_titles.loc[index, 'seniority_level'] = \"Entry-Level / Junior\" \n",
    "        elif row['job_keyword'] == 'Data Scientist' or row['job_keyword'] == 'Data Engineering' or row['job_keyword'] == 'MLOps Engineer' or \\\n",
    "                row['job_keyword'] == 'Business Intelligence' or row['job_keyword'] == 'Data Coordinator' or row['job_keyword'] == 'Data Steward' or \\\n",
    "                row['job_keyword'] == 'Machine Learning Infrastructure Engineer' or row['job_keyword'] == 'Machine Learning Software Developer' or row['job_keyword'] == 'Software Engineer' or \\\n",
    "                row['job_keyword'] == 'Customer Data Scientist' or row['job_keyword'] == 'Data Warehouse Architect'  or row['job_keyword'] == 'Ml Systems' or \\\n",
    "                row['job_keyword'] == 'Data Compliance' or row['job_keyword'] == 'Big Data Architect' or row['job_keyword'] == 'Aws Databricks' or \\\n",
    "                row['job_keyword'] == 'Big Data Developer' or row['job_keyword'] == 'Azure Data' or row['job_keyword'] == 'Data Replication Engineer' or \\\n",
    "                row['job_keyword'] == 'Data Science Practitioner' or row['job_keyword'] == 'Data Integrations' or row['job_keyword'] == 'Data Modeling' or \\\n",
    "                row['job_keyword'] == 'Machine Learning Operations' or row['job_keyword'] == 'Mlops' or row['job_keyword'] == 'Data Loss Prevention' or \\\n",
    "                row['job_keyword'] == 'Ml Infrastructure' or row['job_keyword'] == 'Machine Learning Software Engineer' or row['job_keyword'] == 'Data Deployment' or \\\n",
    "                row['job_keyword'] == 'Data Architecture' or row['job_keyword'] == 'Datacenter Network Engineer' or row['job_keyword'] == 'Azure Databricks' or \\\n",
    "                row['job_keyword'] == 'Data Stewardship' or row['job_keyword'] == 'Ml Platform' or row['job_keyword'] == 'Data Conversion' or \\\n",
    "                row['job_keyword'] == 'Data Management Engineer':\n",
    "            classified_job_titles.loc[index, 'seniority_level'] = \"Mid-Level\"\n",
    "        elif row['job_keyword'] == 'Data Architect' or row['job_keyword'] == 'Data Warehouse' or row['job_keyword'] == 'Cloud Data Architect' or \\\n",
    "                row['job_keyword'] == 'Data Protection' or row['job_keyword'] == 'Data Lake Architect' or row['job_keyword'] == 'Enterprise Data Architect' or \\\n",
    "                row['job_keyword'] == 'Data Solution Architect' or row['job_keyword'] == 'Data Streaming Architect':\n",
    "            classified_job_titles.loc[index, 'seniority_level'] = \"Senior-Level\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Drop unclassified rows**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows\n",
    "classified_job_titles = classified_job_titles[classified_job_titles['job_classification'] != 'unclassified']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create job_classification and job_keyword ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify job titles and create a new DataFrame with classification results\n",
    "classified_job_titles['job_classification'], classified_job_titles['job_keyword'] = zip(*classified_job_titles['job_title'].apply(classify))\n",
    "\n",
    "# Get unique job classifications and assign title_ids\n",
    "unique_classifications = classified_job_titles[['job_classification']].drop_duplicates().reset_index(drop=True)\n",
    "unique_classifications['title_id'] = range(1, len(unique_classifications) + 1)\n",
    "\n",
    "# Get unique job keywords and assign keyword_ids\n",
    "unique_keywords = classified_job_titles[['job_keyword']].drop_duplicates().reset_index(drop=True)\n",
    "unique_keywords['keyword_id'] = range(1, len(unique_keywords) + 1)\n",
    "\n",
    "# Merge the unique classifications back to the original DataFrame\n",
    "classified_job_titles = pd.merge(classified_job_titles, unique_classifications, on='job_classification', how='left')\n",
    "\n",
    "# Merge the unique keywords back to the original DataFrame\n",
    "classified_job_titles = pd.merge(classified_job_titles, unique_keywords, on='job_keyword', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Assign unique job_id to each job posting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a unique identifier to each job posting\\\n",
    "classified_job_titles['job_id'] = [i for i in range(1, len(classified_job_titles) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the job_id the first column\n",
    "column = ['job_id'] + [col for col in classified_job_titles.columns if col != 'job_id']\n",
    "classified_job_titles = classified_job_titles[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>job_link</th>\n",
       "      <th>last_processed_time</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>job_classification</th>\n",
       "      <th>job_keyword</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>seniority_level_keyword</th>\n",
       "      <th>title_id</th>\n",
       "      <th>keyword_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-mach...</td>\n",
       "      <td>2024-01-21 08:08:48.031964+0000</td>\n",
       "      <td>Senior Machine Learning Engineer</td>\n",
       "      <td>Jobs for Humanity</td>\n",
       "      <td>New Haven</td>\n",
       "      <td>CT</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>Senior-Level</td>\n",
       "      <td>Senior</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/principal-s...</td>\n",
       "      <td>2024-01-20 04:02:12.331406+0000</td>\n",
       "      <td>Principal Software Engineer, ML Accelerators</td>\n",
       "      <td>Aurora</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>Software &amp; Platform Engineering</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Principal / Staff-Level</td>\n",
       "      <td>Principal</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-etl-...</td>\n",
       "      <td>2024-01-21 08:08:31.941595+0000</td>\n",
       "      <td>Senior ETL Data Warehouse Specialist</td>\n",
       "      <td>Adame Services LLC</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>Data Modeling &amp; Warehousing</td>\n",
       "      <td>Data Warehouse</td>\n",
       "      <td>Senior-Level</td>\n",
       "      <td>Senior</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-data...</td>\n",
       "      <td>2024-01-20 15:30:55.796572+0000</td>\n",
       "      <td>Senior Data Warehouse Developer / Architect</td>\n",
       "      <td>Morph Enterprise</td>\n",
       "      <td>Harrisburg</td>\n",
       "      <td>PA</td>\n",
       "      <td>Data Modeling &amp; Warehousing</td>\n",
       "      <td>Data Warehouse</td>\n",
       "      <td>Senior-Level</td>\n",
       "      <td>Senior</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/lead-data-e...</td>\n",
       "      <td>2024-01-21 08:08:58.312124+0000</td>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Dice</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5649</th>\n",
       "      <td>5650</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-data...</td>\n",
       "      <td>2024-01-21 07:11:08.769739+0000</td>\n",
       "      <td>Senior Data Scientist - Statistics</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Senior-Level</td>\n",
       "      <td>Senior</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>5651</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-archit...</td>\n",
       "      <td>2024-01-21 08:08:07.523737+0000</td>\n",
       "      <td>Data Architect</td>\n",
       "      <td>General Dynamics Information Technology</td>\n",
       "      <td>St Louis</td>\n",
       "      <td>MO</td>\n",
       "      <td>Data Architect</td>\n",
       "      <td>Data Architect</td>\n",
       "      <td>Senior-Level</td>\n",
       "      <td>unclassified</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>5652</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/corporate-a...</td>\n",
       "      <td>2024-01-19 15:10:41.177008+0000</td>\n",
       "      <td>Corporate AML Alert Investigation Specialist</td>\n",
       "      <td>Glacier Bancorp, Inc.</td>\n",
       "      <td>Kalispell</td>\n",
       "      <td>MT</td>\n",
       "      <td>Risk &amp; Compliance Analytics</td>\n",
       "      <td>Aml</td>\n",
       "      <td>Senior-Level</td>\n",
       "      <td>Specialist</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>5653</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-data...</td>\n",
       "      <td>2024-01-20 15:20:19.036168+0000</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Highnote</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Senior-Level</td>\n",
       "      <td>Senior</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>5654</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-data...</td>\n",
       "      <td>2024-01-19 23:25:28.107523+0000</td>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>CompSource Mutual Insurance Company</td>\n",
       "      <td>Oklahoma City</td>\n",
       "      <td>OK</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Senior-Level</td>\n",
       "      <td>Senior</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5654 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      job_id                                           job_link  \\\n",
       "0          1  https://www.linkedin.com/jobs/view/senior-mach...   \n",
       "1          2  https://www.linkedin.com/jobs/view/principal-s...   \n",
       "2          3  https://www.linkedin.com/jobs/view/senior-etl-...   \n",
       "3          4  https://www.linkedin.com/jobs/view/senior-data...   \n",
       "4          5  https://www.linkedin.com/jobs/view/lead-data-e...   \n",
       "...      ...                                                ...   \n",
       "5649    5650  https://www.linkedin.com/jobs/view/senior-data...   \n",
       "5650    5651  https://www.linkedin.com/jobs/view/data-archit...   \n",
       "5651    5652  https://www.linkedin.com/jobs/view/corporate-a...   \n",
       "5652    5653  https://www.linkedin.com/jobs/view/senior-data...   \n",
       "5653    5654  https://www.linkedin.com/jobs/view/senior-data...   \n",
       "\n",
       "                  last_processed_time  \\\n",
       "0     2024-01-21 08:08:48.031964+0000   \n",
       "1     2024-01-20 04:02:12.331406+0000   \n",
       "2     2024-01-21 08:08:31.941595+0000   \n",
       "3     2024-01-20 15:30:55.796572+0000   \n",
       "4     2024-01-21 08:08:58.312124+0000   \n",
       "...                               ...   \n",
       "5649  2024-01-21 07:11:08.769739+0000   \n",
       "5650  2024-01-21 08:08:07.523737+0000   \n",
       "5651  2024-01-19 15:10:41.177008+0000   \n",
       "5652  2024-01-20 15:20:19.036168+0000   \n",
       "5653  2024-01-19 23:25:28.107523+0000   \n",
       "\n",
       "                                         job_title  \\\n",
       "0                 Senior Machine Learning Engineer   \n",
       "1     Principal Software Engineer, ML Accelerators   \n",
       "2             Senior ETL Data Warehouse Specialist   \n",
       "3      Senior Data Warehouse Developer / Architect   \n",
       "4                               Lead Data Engineer   \n",
       "...                                            ...   \n",
       "5649            Senior Data Scientist - Statistics   \n",
       "5650                                Data Architect   \n",
       "5651  Corporate AML Alert Investigation Specialist   \n",
       "5652                         Senior Data Scientist   \n",
       "5653                          Senior Data Engineer   \n",
       "\n",
       "                                      company           City State  \\\n",
       "0                           Jobs for Humanity      New Haven    CT   \n",
       "1                                      Aurora  San Francisco    CA   \n",
       "2                          Adame Services LLC       New York    NY   \n",
       "3                            Morph Enterprise     Harrisburg    PA   \n",
       "4                                        Dice          Plano    TX   \n",
       "...                                       ...            ...   ...   \n",
       "5649                          United Airlines        Chicago    IL   \n",
       "5650  General Dynamics Information Technology       St Louis    MO   \n",
       "5651                    Glacier Bancorp, Inc.      Kalispell    MT   \n",
       "5652                                 Highnote  San Francisco    CA   \n",
       "5653      CompSource Mutual Insurance Company  Oklahoma City    OK   \n",
       "\n",
       "                   job_classification                job_keyword  \\\n",
       "0           Machine Learning Engineer  Machine Learning Engineer   \n",
       "1     Software & Platform Engineering          Software Engineer   \n",
       "2         Data Modeling & Warehousing             Data Warehouse   \n",
       "3         Data Modeling & Warehousing             Data Warehouse   \n",
       "4                       Data Engineer         Lead Data Engineer   \n",
       "...                               ...                        ...   \n",
       "5649                   Data Scientist             Data Scientist   \n",
       "5650                   Data Architect             Data Architect   \n",
       "5651      Risk & Compliance Analytics                        Aml   \n",
       "5652                   Data Scientist             Data Scientist   \n",
       "5653                    Data Engineer       Senior Data Engineer   \n",
       "\n",
       "              seniority_level seniority_level_keyword  title_id  keyword_id  \n",
       "0                Senior-Level                  Senior         1           1  \n",
       "1     Principal / Staff-Level               Principal         2           2  \n",
       "2                Senior-Level                  Senior         3           3  \n",
       "3                Senior-Level                  Senior         3           3  \n",
       "4                        Lead                    Lead         4           4  \n",
       "...                       ...                     ...       ...         ...  \n",
       "5649             Senior-Level                  Senior        10          16  \n",
       "5650             Senior-Level            unclassified        11          30  \n",
       "5651             Senior-Level              Specialist         5          41  \n",
       "5652             Senior-Level                  Senior        10          16  \n",
       "5653             Senior-Level                  Senior         4           5  \n",
       "\n",
       "[5654 rows x 13 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_job_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OUTPUT Cleaned CSVs** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files successfully exported to data directory\n"
     ]
    }
   ],
   "source": [
    "# Output cleaned job postings data\n",
    "classified_job_titles.to_csv('../data/cleaned_job_postings.csv', index=False)\n",
    "\n",
    "# Output cleaned job skills data \n",
    "job_skills.to_csv('../data/cleaned_job_skills.csv', index=False)\n",
    "\n",
    "print(\"CSV files successfully exported to data directory\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
